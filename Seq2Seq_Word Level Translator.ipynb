{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk4S2XfoLqFE"
   },
   "source": [
    "### Seq2Seq) Word Level 번역기 만들기\n",
    "\n",
    "-  데이터셋 : 영어 및 프랑스어 문장 벙렬 코퍼스 활용 (http://www.manythings.org/anki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded to ./data/fra-eng.zip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src         tar\n",
       "0  Go.        Va !\n",
       "1  Go.     Marche.\n",
       "2  Go.  En route !\n",
       "3  Go.     Bouge !\n",
       "4  Hi.     Salut !"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### 1. 데이터 불러오기\n",
    "# http://www.manythings.org/anki 내 fra-eng.zip이라는 영어 - 프랑스어 병렬 코퍼스 활용\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "output_path = \"./data/fra-eng.zip\"\n",
    "download_zip(url, output_path)\n",
    "\n",
    "# 압축 파일 경로 설정\n",
    "extract_path = \"./data/\"\n",
    "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)  # ./Data/에 압축 해제\n",
    "\n",
    "# 파일 읽기 (경로 수정)\n",
    "lines = pd.read_csv(os.path.join(extract_path, 'fra.txt'), names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "\n",
    "# 데이터 확인\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "\n",
    "1. 프랑스어 코퍼스의 악센트 및 영어 외 문자 제거\n",
    "2. 기존 target data에 시작 토큰 \"\\<sos>\" 및 끝 토큰 \"\\<eos>\"을 추가한 뒤 띄어쓰기를 기준으로 split\n",
    "3. 띄어쓰기된 Word 단위로 정수 인코딩을 진행하여 Encoder Input을 생성\n",
    "4. 동일하게 띄어쓰기된 Word 단위로 정수 인코딩 진행하여 Decoder Input를 생섬 및 시작 토큰 \"\\<eos>\"를 제외한 Decoder Target을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BiiuXl3FL6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "### 전처리 진행\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Turn a Unicode string to plain ASCII\n",
    "def to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def preprocess_sentence(sent):\n",
    "\n",
    "  sent = to_ascii(sent.lower())\n",
    "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "  # 다수 개의 공백을 하나의 공백으로 치환\n",
    "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "  return sent\n",
    "\n",
    "\n",
    "# 데이터셋 전처리 함수 : Encoder Input, Decoder Input, Decoder Target 반환\n",
    "def load_preprocessed_data():\n",
    "  encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "  with open(\"./data/fra.txt\", \"r\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "\n",
    "      src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "      # source 데이터 전처리\n",
    "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "      # target 데이터 전처리\n",
    "      tar_line = preprocess_sentence(tar_line)\n",
    "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "      encoder_input.append(src_line)\n",
    "      decoder_input.append(tar_line_in)\n",
    "      decoder_target.append(tar_line_out)\n",
    "\n",
    "      if i == num_samples - 1:\n",
    "        break\n",
    "\n",
    "  return encoder_input, decoder_input, decoder_target\n",
    "\n",
    "\n",
    "# 전처리 진행\n",
    "num_samples = 33000\n",
    "\n",
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation\n",
    "\n",
    "1. Encoder Input\n",
    "2. Decoder Input : <sos> 포함된 Decoder training용 input(교사 강요)\n",
    "3. Decoder Target : <eos> 포함된 Decoder accuracy 측정을 위한 정답 ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NHmR03JJNR_Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33000/33000 [00:00<00:00, 4022319.37it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 3339494.58it/s]\n",
      "100%|██████████| 33000/33000 [00:00<00:00, 410157.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 7)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n",
      "영어 단어 집합 크기: 4486, 프랑스어 단어 집합 크기: 7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. 단어 사전 구축 함수\n",
    "def build_vocab(sents):\n",
    "  word_list = []\n",
    "\n",
    "  for sent in sents:\n",
    "      for word in sent:\n",
    "        word_list.append(word)\n",
    "\n",
    "  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n",
    "  word_counts = Counter(word_list)\n",
    "  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "  word_to_index = {}\n",
    "  word_to_index['<PAD>'] = 0\n",
    "  word_to_index['<UNK>'] = 1\n",
    "\n",
    "  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n",
    "  for index, word in enumerate(vocab) :\n",
    "    word_to_index[word] = index + 2\n",
    "\n",
    "  return word_to_index\n",
    "\n",
    "\n",
    "# 단어 사전 구축 및 size 측정\n",
    "src_vocab = build_vocab(sents_en_in)\n",
    "tar_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tar_vocab_size = len(tar_vocab)\n",
    "\n",
    "\n",
    "\n",
    "# 2. word2index : word 사전을 index화\n",
    "index_to_src = {v: k for k, v in src_vocab.items()}\n",
    "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
    "\n",
    "def texts_to_sequences(sents, word_to_index):\n",
    "  encoded_X_data = []\n",
    "  for sent in tqdm(sents):\n",
    "    index_sequences = []\n",
    "    for word in sent:\n",
    "      try:\n",
    "          index_sequences.append(word_to_index[word])\n",
    "      except KeyError:\n",
    "          index_sequences.append(word_to_index['<UNK>'])\n",
    "    encoded_X_data.append(index_sequences)\n",
    "  return encoded_X_data\n",
    "\n",
    "\n",
    "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
    "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
    "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Padding : 최대 길이에 맞춰서 padding 적용\n",
    "def pad_sequences(sentences, max_len=None):\n",
    "    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
    "    if max_len is None:\n",
    "        max_len = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input)\n",
    "decoder_input = pad_sequences(decoder_input)\n",
    "decoder_target = pad_sequences(decoder_target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n",
    "print(f'영어 단어 집합 크기: {str(src_vocab_size)}, 프랑스어 단어 집합 크기: {str(tar_vocab_size)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train / Test split\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = int(33000*0.1)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "\n",
    "\n",
    "#### Tensor Dataset 설정 -> Pytorch Dataset 변환\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxRngmqaOZ2r"
   },
   "source": [
    "#### Training\n",
    "\n",
    "- 학습 방법\n",
    "    1. Encoder을 Encoder Input을 이용해서 학습 진행\n",
    "    2. Encoder 내 LSTM 셀 중 마지막 출력을 Decoder의 Initial State로 활용\n",
    "    3. Decoder을 Decoder Input을 활용하여 학습 진행\n",
    "    4. Decoder의 결과와 Decoder Target 사이의 Cross Entropy Loss로 Backpropagation 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3-WEpOMPOam9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 정보\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(4486, 256, padding_idx=0)\n",
      "    (lstm): LSTM(256, 256, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(7879, 256, padding_idx=0)\n",
      "    (lstm): LSTM(256, 256, batch_first=True)\n",
      "    (fc): Linear(in_features=256, out_features=7879, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Device 설정 (CUDA 사용 가능하면 GPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 매개변수 설정\n",
    "embedding_dim = 256\n",
    "hidden_units = 256\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# 1. Encoder 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0) # Padding 인덱스 설정\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)   # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # 인코더의 출력은 hidden state, cell state\n",
    "        return hidden, cell\n",
    "\n",
    "# 2. Decoder 정의\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0) # Padding 인덱스 설정\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embedding(x)   # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # output.shape == (batch_size, seq_len, hidden_units) | hidden.shape == (1, batch_size, hidden_units) | cell.shape == (1, batch_size, hidden_units)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        output = self.fc(output)    # (batch_size, seq_len, tar_vocab_size)\n",
    "\n",
    "        # 디코더의 출력은 예측값, hidden state, cell state\n",
    "        return output, hidden, cell\n",
    "    \n",
    "\n",
    "# 3. Seq2Seq 모델 정의\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        hidden, cell = self.encoder(encoder_input)\n",
    "        outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n",
    "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model 정보\")\n",
    "print(model)\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Train Loss: 2.9462 | Train Acc: 0.5296 | Valid Loss: 3.0746 | Valid Acc: 0.5267\n",
      "Epoch: 2/10 | Train Loss: 2.2702 | Train Acc: 0.6028 | Valid Loss: 2.5088 | Valid Acc: 0.5910\n",
      "Epoch: 3/10 | Train Loss: 1.8591 | Train Acc: 0.6467 | Valid Loss: 2.2164 | Valid Acc: 0.6206\n",
      "Epoch: 4/10 | Train Loss: 1.5585 | Train Acc: 0.6835 | Valid Loss: 2.0304 | Valid Acc: 0.6418\n",
      "Epoch: 5/10 | Train Loss: 1.2996 | Train Acc: 0.7201 | Valid Loss: 1.8792 | Valid Acc: 0.6604\n",
      "Epoch: 6/10 | Train Loss: 1.0843 | Train Acc: 0.7586 | Valid Loss: 1.7636 | Valid Acc: 0.6793\n",
      "Epoch: 7/10 | Train Loss: 0.9019 | Train Acc: 0.7942 | Valid Loss: 1.6698 | Valid Acc: 0.6908\n",
      "Epoch: 8/10 | Train Loss: 0.7436 | Train Acc: 0.8287 | Valid Loss: 1.6059 | Valid Acc: 0.7012\n",
      "Epoch: 9/10 | Train Loss: 0.6230 | Train Acc: 0.8503 | Valid Loss: 1.5710 | Valid Acc: 0.7074\n",
      "Epoch: 10/10 | Train Loss: 0.5218 | Train Acc: 0.8755 | Valid Loss: 1.5327 | Valid Acc: 0.7144\n"
     ]
    }
   ],
   "source": [
    "# 4. training by epoch & loss update\n",
    "def evaluation(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "            # 순방향 전파\n",
    "            # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
    "            outputs = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "            # 손실 계산\n",
    "            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
    "            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산 (패딩 토큰 제외)\n",
    "            mask = decoder_targets != 0\n",
    "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count\n",
    "\n",
    "\n",
    "\n",
    "############################### 5. Training ##################################\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train_seq2seq(model, train_dataloader, valid_dataloader, optimizer, loss_function, device, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains a Seq2Seq model using the provided dataloaders, optimizer, and loss function.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model (Seq2Seq)\n",
    "        train_dataloader: DataLoader for training data\n",
    "        valid_dataloader: DataLoader for validation data\n",
    "        optimizer: Optimizer for training\n",
    "        loss_function: Loss function\n",
    "        device: Device to run the model on (\"cuda\" or \"cpu\")\n",
    "        num_epochs: Number of epochs to train\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(encoder_inputs, decoder_inputs)\n",
    "            \n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "        valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_seq2seq(model, train_dataloader, valid_dataloader, optimizer, loss_function, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 1.5327\n",
      "Best model validation accuracy: 0.7144\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 정확도와 손실 계산\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9ys5pYzPFzP"
   },
   "source": [
    "#### Translator Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "H9grVuQfPFkP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : i am good . \n",
      "정답문장 : je suis bon . \n",
      "번역문장 : je suis bon .\n",
      "--------------------------------------------------\n",
      "입력문장 : i m getting old . \n",
      "정답문장 : je commence a devenir vieux . \n",
      "번역문장 : je me fais vieux .\n",
      "--------------------------------------------------\n",
      "입력문장 : this is my horse . \n",
      "정답문장 : ce cheval est a moi . \n",
      "번역문장 : c est mon cheval .\n",
      "--------------------------------------------------\n",
      "입력문장 : we have no proof . \n",
      "정답문장 : nous n avons aucune preuve . \n",
      "번역문장 : nous n avons aucune preuve .\n",
      "--------------------------------------------------\n",
      "입력문장 : you re very wise . \n",
      "정답문장 : vous etes tres avises . \n",
      "번역문장 : vous etes tres sages .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#### Inference\n",
    "\n",
    "index_to_src = {v: k for k, v in src_vocab.items()}\n",
    "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n",
    "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n",
    "    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # 인코더의 초기 상태 설정\n",
    "    hidden, cell = model.encoder(encoder_inputs)\n",
    "\n",
    "    # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n",
    "    # unsqueeze(0)는 배치 차원을 추가하기 위함.\n",
    "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    decoded_tokens = []\n",
    "\n",
    "    # for문을 도는 것 == 디코더의 각 시점\n",
    "    for _ in range(max_output_len):\n",
    "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n",
    "        output_token = output.argmax(dim=-1).item()\n",
    "\n",
    "        # 종료 토큰 <eos>\n",
    "        if output_token == 4:\n",
    "            break\n",
    "\n",
    "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n",
    "        decoded_tokens.append(output_token)\n",
    "\n",
    "        # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n",
    "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    return ' '.join(int_to_tar_token[token] for token in decoded_tokens)\n",
    "\n",
    "\n",
    "\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index]\n",
    "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",translated_text)\n",
    "  print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSzXG95mvWOlrF9e/jYSym",
   "collapsed_sections": [
    "u77tpXhrCnlq"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP_Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
